from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec
import numpy
import numpy as np
from random import shuffle
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation,Merge
from keras.optimizers import SGD
from scipy.interpolate import spline
from keras.callbacks import LearningRateScheduler
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from keras.layers.normalization import BatchNormalization
from sklearn.preprocessing import MinMaxScaler

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources
        
        flipped = {}
        
        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')
    
    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])
    
    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
        return self.sentences
    
    def sentences_perm(self):
        shuffle(self.sentences)
        return self.sentences
        
sources = {'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS', 'test-pos.txt':'TEST_POS','test-neg.txt':'TEST_NEG'}

sentences = LabeledLineSentence(sources)
sentences
model = Doc2Vec(min_count=1, window=5, size=10, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())

for epoch in range(10):
    model.train(sentences.sentences_perm())
    
model.save('./imdb.d2v')

model = Doc2Vec.load('./imdb.d2v')
model
model.most_similar('good')

sentences.to_array()
model.docvecs['TRAIN_NEG_7']

train_arrays = []
train_labels = []
train_arrays0 = []
train_labels0 = []

for i in range(6):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays.append(model.docvecs[prefix_train_pos])
    train_labels.append(1)

for i in range(6):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays0.append(model.docvecs[prefix_train_neg])
    train_labels0.append(0)

test_arrays = []
test_labels = []
test_arrays0 = []
test_labels0 = []

for i in range(6):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays.append(model.docvecs[prefix_test_pos])
    test_labels.append(1)

for i in range(6):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays0.append(model.docvecs[prefix_test_neg])
    test_labels0.append(0)

sd=[]
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = [1,1]

    def on_epoch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        sd.append(step_decay(len(self.losses)))
        print('lr:', step_decay(len(self.losses)))

def step_decay(losses):
    if float(2*np.sqrt(np.array(history.losses[-1])))<0.23:
        lrate=0.06
        momentum=0.3
        decay_rate=2e-6
        return lrate
    else:
        lrate=0.06
        return lrate

learning_rate = 0.07
decay_rate = 5e-6
momentum = 0.7

scaler = MinMaxScaler(feature_range=(0, 1))

X_train_left=scaler.fit_transform(np.array(train_arrays))
X_train_right=scaler.fit_transform(np.array(train_arrays0))

X_test_left=scaler.fit_transform(np.array(test_arrays))
X_test_right=scaler.fit_transform(np.array(test_arrays))
y_test=[1,1,1,0,0,0]

Y_train_concat=[1,1,1,0,0,0]

model_left=Sequential()
model_left.add(Dense(3, input_dim=10, init='uniform'))
model_left.add(Dense(3))

model_right=Sequential()
model_right.add(Dense(3, input_dim=10, init='uniform'))
model_right.add(Dense(3))

model3=Sequential()
model3.add(Merge([model_left,model_right],mode = 'concat'))
model3.add(Dense(3, init = 'uniform'))
model3.add(Activation('sigmoid'))
model3.add(Dense(1))
model3.add(Activation('sigmoid'))
sgd = SGD(lr=learning_rate,momentum=momentum, decay=decay_rate, nesterov=False)
model3.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])

model3.fit([X_train_left, X_train_right], Y_train_concat, 
           batch_size = 6, nb_epoch = 500, verbose = 1)

# 1
res = model3.predict_classes([X_train_left,X_train_left])
res

for i in range(0,len(res)):
    if res[i]>.5:
        res[i]=1
    else:
        res[i]=0

p=[]
for i in range(0,6):
    if res[i]==0:
        p.append('Negative')
    else:
        p.append('Positive')

p2=[]
for i in range(0,6):
    if y_test[i]==0:
        p2.append('Negative')
    else:
        p2.append('Positive')

from bs4 import BeautifulSoup
import nltk
import numpy as np
from nltk import sent_tokenize, word_tokenize, pos_tag

filename = "test-pos.txt"
raw_text2 = open(filename).read()
html2=raw_text2
soup = BeautifulSoup(html2,"lxml")

###### SEMANTIC
texto=[]
for string in soup.stripped_strings:
    texto.append(repr(string))

texto

# kill all script and style elements
for script in soup(["script", "style"]):
    script.extract()    # rip it out

# get text
text2 = soup.get_text()
text2

sentences_pos = sent_tokenize(text2)
sentences_pos

filename = "test-neg2.txt"
raw_text2 = open(filename).read()
html=raw_text2
soup2 = BeautifulSoup(html,"lxml")
html

# get text
text2 = soup2.get_text()
text2
sentences_neg = sent_tokenize(text2)

sentences=sentences_neg

print('ACTUAL SENTIMENT:','\n',p2,'\n')
print('PREDICTED SENTIMENT:','\n',p)
